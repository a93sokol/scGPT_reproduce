{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pre-trained model and extracting information about context specific feature impacts using Sampling Perturbation method for Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpcfs/users/a1234104/miniconda3/envs/scGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_relative_error,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
    "\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 0\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "amp = True\n",
    "load_model = \"../save/scGPT_human\"\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "    \"transformer_decoder\"\n",
    "]\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "epochs = 3\n",
    "schedule_interval = 1\n",
    "early_stop = 10\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0  # dropout probability\n",
    "use_fast_transformer = True  # whether to use fast transformer\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loader': <torch_geometric.deprecation.DataLoader at 0x7fc7e1ff7640>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data = PertData(\"../data/\")\n",
    "pert_data.load(data_path='../data/fibroblast_fm6_with_fm1_raw/')\n",
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to save/dev_perturb_testing_sampling_pert-Nov06-08-23\n",
      "scGPT - INFO - Running on 2024-11-06 08:23:24\n"
     ]
    }
   ],
   "source": [
    "data_name = 'testing_sampling_pert'\n",
    "save_dir = Path(f\"./save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"saving to {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "# log running date and current git commit\n",
    "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 17315/17315 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../save/scGPT_human/best_model.pt, the model args will override the config ../save/scGPT_human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if load_model is not None:\n",
    "    model_dir = Path(load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    pert_data.adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Create and train scGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scgpt.model import TransformerModel\n",
    "from scgpt.utils import load_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "# if load_param_prefixs is not None and load_model is not None:\n",
    "#     # only load params that start with the prefix\n",
    "#     model_dict = model.state_dict()\n",
    "#     pretrained_dict = torch.load(model_file,  map_location=torch.device(device))\n",
    "#     pretrained_dict = {\n",
    "#         k: v\n",
    "#         for k, v in pretrained_dict.items()\n",
    "#         if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
    "#     }\n",
    "#     for k, v in pretrained_dict.items():\n",
    "#         logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "#     model_dict.update(pretrained_dict)\n",
    "#     model.load_state_dict(model_dict)\n",
    "# elif load_model is not None:\n",
    "#     try:\n",
    "#         model.load_state_dict(torch.load(model_file))\n",
    "#         logger.info(f\"Loading all model params from {model_file}\")\n",
    "#     except:\n",
    "#         # only load params that are in the model and match the size\n",
    "#         model_dict = model.state_dict()\n",
    "#         pretrained_dict = torch.load(model_file, map_location=torch.device(device))\n",
    "#         pretrained_dict = {\n",
    "#             k: v\n",
    "#             for k, v in pretrained_dict.items()\n",
    "#             if k in model_dict and v.shape == model_dict[k].shape\n",
    "#         }\n",
    "#         for k, v in pretrained_dict.items():\n",
    "#             logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "#         model_dict.update(pretrained_dict)\n",
    "#         model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading parameter encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading parameter encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.0.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.0.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.4.weight with shape torch.Size([1, 512])\n",
      "scGPT - INFO - Loading parameter decoder.fc.4.bias with shape torch.Size([1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (8): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (9): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (10): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (11): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = load_pretrained(model, torch.load(model_file), verbose=True, strict=False)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_array(n_masks, n_genes, fraction):\n",
    "    \"\"\"\n",
    "    Generate a binary 2D NumPy array with a set fraction of 1s randomly distributed in every row.\n",
    "\n",
    "    Parameters:\n",
    "    rows (int): Number of rows in the array.\n",
    "    cols (int): Number of columns in the array.\n",
    "    fraction (float): Fraction of 1s in each row (0 <= fraction <= 1).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Binary 2D NumPy array.\n",
    "    \"\"\"\n",
    "    if not (0 <= fraction <= 1):\n",
    "        raise ValueError(\"Fraction must be between 0 and 1.\")\n",
    "\n",
    "    array = np.zeros((n_masks, n_genes), dtype=int)\n",
    "    num_ones = int(fraction * n_genes)\n",
    "\n",
    "    for row in array:\n",
    "        ones_indices = np.random.choice(n_genes, num_ones, replace=False)\n",
    "        row[ones_indices] = 1\n",
    "\n",
    "    return array\n",
    "\n",
    "def apply_masks(\n",
    "    values: Union[torch.Tensor, np.ndarray],\n",
    "    masks: np.ndarray,\n",
    "    mask_value: int = -1,\n",
    "    pad_value: int = 0,\n",
    "    indices_to_keep: List[int] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply given masks to the values.\n",
    "\n",
    "    Args:\n",
    "        values (array-like): The data to mask, shape (n_features,)\n",
    "        masks (array-like): An array of masks to apply, shape (n_masks, n_features)\n",
    "        mask_value (int): The value to mask with, default to -1.\n",
    "        pad_value (int): The value of padding in the values, will be kept unchanged.\n",
    "        indices_to_keep (list of int): List of indices that should not be masked.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of masked data, shape (n_masks, n_features)\n",
    "    \"\"\"\n",
    "    if isinstance(values, torch.Tensor):\n",
    "        values = values.clone().detach().cpu().numpy()\n",
    "    else:\n",
    "        values = values.copy()\n",
    "\n",
    "    if indices_to_keep is None:\n",
    "        indices_to_keep = []\n",
    "\n",
    "    masked_values = []\n",
    "    for i in range(masks.shape[0]):\n",
    "        mask = masks[i]\n",
    "        masked_value = values.copy()\n",
    "        # Only mask the positions where mask == 1 and values != pad_value\n",
    "        mask_positions = (mask == 1) & (values != pad_value)\n",
    "        # Set the indices that should not be masked to False\n",
    "        mask_positions[indices_to_keep] = False\n",
    "        masked_value[mask_positions] = mask_value\n",
    "        masked_values.append(masked_value)\n",
    "\n",
    "    return torch.from_numpy(np.array(masked_values)).float()\n",
    "\n",
    "def sample_pert(\n",
    "    model: nn.Module,\n",
    "    cell_data: torch_geometric.data.Data,\n",
    "    masks: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies multiple masks to a single cell record, reconstructs the missing values using the model,\n",
    "    and returns the reconstructed cell expressions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The transformer model.\n",
    "        cell_data (torch_geometric.data.Data): The data of a single cell.\n",
    "        masks (np.ndarray): An array of masks to apply, shape (n_masks, n_genes).\n",
    "        device (torch.device): The device to run the computations on.\n",
    "        map_raw_id_to_vocab_id (Callable): Function to map raw gene IDs to vocab IDs.\n",
    "        gene_ids (torch.Tensor): Tensor of gene IDs.\n",
    "        criterion (Callable): Loss function.\n",
    "        amp (bool): Automatic Mixed Precision flag.\n",
    "        CLS, CCE, MVC, ECS: Model-specific flags.\n",
    "        vocab (Dict): Vocabulary mapping.\n",
    "        pad_token (str): Padding token.\n",
    "        max_seq_len (int, optional): Maximum sequence length.\n",
    "        include_zero_gene (str): Inclusion criteria for zero-expression genes.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reconstructed cell expressions, shape (n_masks, n_genes).\n",
    "    \"\"\"\n",
    "    #initial_state_dict = model.state_dict()\n",
    "    \n",
    "    model.eval()\n",
    "    cell_data.to(device)\n",
    "    x: torch.Tensor = cell_data.x.reshape(1,-1)  # (n_genes, 2)\n",
    "    \n",
    "    ori_gene_values = x[0, :]  # (n_genes,)\n",
    "    \n",
    "    target_gene_values = x  # (n_genes,)\n",
    "\n",
    "    n_masks = masks.shape[0]\n",
    "    n_genes = target_gene_values.shape[1]\n",
    "\n",
    "    # Repeat ori_gene_values and pert_flags for each mask\n",
    "    \n",
    "    #ori_gene_values = ori_gene_values.unsqueeze(0).repeat(n_masks, 1)  # (n_masks, n_genes)\n",
    "        \n",
    "    # Prepare input_gene_ids\n",
    "    if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "        if include_zero_gene == \"all\":\n",
    "            input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            input_gene_ids = (\n",
    "                ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "            )\n",
    "        # Sample input_gene_id\n",
    "        if max_seq_len and len(input_gene_ids) > max_seq_len:\n",
    "            # input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "            #     :max_seq_len\n",
    "            # ]\n",
    "            input_gene_ids = torch.tensor(np.arange(max_seq_len)).to(device)\n",
    "\n",
    "        \n",
    "        input_values = ori_gene_values[input_gene_ids]\n",
    "    \n",
    "        input_values = apply_masks(\n",
    "                                        input_values,\n",
    "                                        masks[:, input_gene_ids.cpu().numpy()],\n",
    "                                        mask_value=-1,\n",
    "                                        pad_value=0,\n",
    "                                    ) \n",
    "    \n",
    "        \n",
    "        mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "        mapped_input_gene_ids = mapped_input_gene_ids.unsqueeze(0).repeat(n_masks, 1)\n",
    "\n",
    "        # src_key_padding_mask\n",
    "        src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
    "    \n",
    "    \n",
    "    with torch.cuda.amp.autocast(enabled=amp):\n",
    "        output_dict = model(\n",
    "            mapped_input_gene_ids.to(device),\n",
    "            input_values.to(device),\n",
    "            src_key_padding_mask=src_key_padding_mask.to(device),\n",
    "            CLS=CLS,\n",
    "            CCE=CCE,\n",
    "            MVC=MVC,\n",
    "            ECS=ECS,\n",
    "        )\n",
    "        output_values = output_dict[\"mlm_output\"]  # (n_masks, n_genes)\n",
    "        #print(output_dict[\"mlm_output\"].shape)\n",
    "    # Reset model to initial state\n",
    "    #model.load_state_dict(initial_state_dict)\n",
    "    return output_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the reconstructed reprogramming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKSM = ['SOX2', 'KLF4', 'POU5F1', 'MYC', 'NANOG']\n",
    "\n",
    "TFs = pd.read_csv('../../perturb_train/little_data/TF_db.csv', index_col = 0)\n",
    "TFs = TFs.loc[:,'HGNC symbol'].tolist()\n",
    "\n",
    "ipsc_genes = pert_data.adata.var.gene_name.tolist()\n",
    "to_perturb = list(set(ipsc_genes).intersection(set(TFs)))\n",
    "\n",
    "tf_exp = pert_data.adata[:,pert_data.adata.var.gene_name.isin(to_perturb)].X\n",
    "\n",
    "a = (tf_exp.shape[0] - (tf_exp==0).sum(axis=0))/tf_exp.shape[0]\n",
    "threshold = 0.10  # Set your desired threshold here\n",
    "\n",
    "# Get the indexes where values are above the threshold\n",
    "to_perturb = np.array(to_perturb)[np.array(a).squeeze() > threshold]\n",
    "to_perturb = np.array(list(set(list(to_perturb) + OKSM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_list = []\n",
    "# for sample in ['fm6', 'mix', 'pr2', 'nr3', 'nic']:\n",
    "#     pert_data.load(data_path = f'../data/fibroblast_{sample}_with_fm1_raw')\n",
    "#     pert_data.adata.obs.loc[pert_data.adata.obs.condition != 'ctrl','cluster'] = sample\n",
    "#     pert_data.adata.obs.loc[pert_data.adata.obs.condition == 'ctrl','cluster'] = 'fm1'\n",
    "#     adata_list.append(pert_data.adata)\n",
    "\n",
    "# pert_data.prepare_split(split='no_split', seed=1)\n",
    "# pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size,)\n",
    "# import anndata as ad    \n",
    "# adata_combined = ad.concat(adata_list, join='outer', axis=0)\n",
    "\n",
    "# duplicates = adata_combined.obs.index.duplicated(keep='first')\n",
    "\n",
    "# adata_combined = adata_combined[~duplicates,adata_combined.var.index.isin(to_perturb)].copy()\n",
    "\n",
    "# adata_combined.write_h5ad('./save/adata_combined.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "adata_combined = sc.read_h5ad('./save/adata_combined.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_combined.X = adata_combined.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_raw = True\n",
    "n_hvg = False\n",
    "n_bins = 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scgpt.preprocess import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - WARNING - The input data seems to be already log1p transformed. Set `log1p=False` to avoid double log1p transform.\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data following the scGPT data pre-processing pipeline\n",
    "preprocessor = Preprocessor(\n",
    "    use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "    filter_gene_by_counts=1,  # step 1\n",
    "    filter_cell_by_counts=False,  # step 2\n",
    "    normalize_total=1e4,  # 3. whether to normalize the raw data and to what sum\n",
    "    result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "    log1p=data_is_raw,  # 4. whether to log1p the normalized data\n",
    "    result_log1p_key=\"X_log1p\",\n",
    "    subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "    hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "    binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "    result_binned_key=\"X\",  # the key in adata.layers to store the binned data\n",
    ")\n",
    "r = preprocessor(adata_combined,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0, 26, ...,  0,  0,  0],\n",
       "       [ 0,  0, 29, ...,  0,  0,  0],\n",
       "       [ 0,  0, 44, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  5, ...,  0,  0, 19],\n",
       "       [40,  0, 32, ...,  0,  0, 39],\n",
       "       [ 0,  0,  0, ...,  0,  0, 37]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_combined.layers['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "to_perturb = adata_combined.var.index.values.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train_loader = pert_data.dataloader[\"train_loader\"]\n",
    "# n_masks = 12500\n",
    "# batch = 128\n",
    "# fraction = 0.15\n",
    "# libs = {}\n",
    "# for library in ['D0-fm', 'D20-nr', 'P20-nr', 'D8-fm', 'D4-fm', 'P3-nr']:\n",
    "#     expression = adata_combined[adata_combined.obs.library==library].X[0].toarray()\n",
    "#     single_cell_ref = torch.tensor(expression)\n",
    "#     n_genes = single_cell_ref.shape[1]\n",
    "#     masks = generate_binary_array(n_masks, n_genes, fraction)\n",
    "    \n",
    "#     ref_cells = []\n",
    "#     for i in range(0, masks.shape[0]-batch, batch):\n",
    "#         res = sample_pert(model, Data(x=single_cell_ref), masks[i:i+batch]).detach().cpu().numpy()\n",
    "#         ref_cells.append(res)\n",
    "        \n",
    "#     ref_cells = np.array(ref_cells).reshape(-1, len(to_perturb))\n",
    "#     ref_cells = pd.DataFrame(ref_cells, columns = adata_combined.var.index.tolist())\n",
    "#     libs[library] = ref_cells.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optimise the number of masks needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dimension_fluctuations(model, adata, batch=4, fraction=0.3, step_size=64):\n",
    "    \"\"\"\n",
    "    Analyze how the first 10 dimensions fluctuate with increasing mask sizes.\n",
    "\n",
    "    Parameters:\n",
    "    - single_cell_pert: numpy array, perturbed cell data\n",
    "    - model: PyTorch model used to sample perturbations\n",
    "    - adata: AnnData object containing experimental data\n",
    "    - batch: int, batch size for sampling perturbations\n",
    "    - fraction: float, fraction of genes to perturb\n",
    "    - step_size: int, increment in number of masks for each iteration\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    n_genes = adata.var.shape[0]\n",
    "    single_cell_pert = adata.X[0].toarray()\n",
    "    \n",
    "    \n",
    "    max_masks = 100000\n",
    "    mask_sizes = list(range(step_size, max_masks + 1, step_size))\n",
    "\n",
    "    # Initialize a list to hold dimension values for each mask size\n",
    "    dimension_values = [[] for _ in range(10)]\n",
    "\n",
    "    pert = adata[adata.obs.condition != 'ctrl'].X.toarray()\n",
    "\n",
    "    # Generate the maximum number of masks once\n",
    "    masks = generate_binary_array(max_masks, n_genes, fraction)\n",
    "\n",
    "    # Sample perturbed cells using the maximum number of masks\n",
    "    pert_cells = []\n",
    "    for i in range(0, masks.shape[0] - batch, batch):\n",
    "        res_pert = sample_pert(model, Data(x=torch.tensor(single_cell_pert)), masks[i:i + batch]).detach().cpu().numpy()\n",
    "        pert_cells.append(res_pert)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    pert_cells = np.array(pert_cells).reshape(-1, single_cell_pert.shape[1])\n",
    "    pert_cells = pd.DataFrame(pert_cells, columns=adata.var.index.tolist())\n",
    "\n",
    "    # Calculate and store the mean values of the first 10 dimensions for each mask size\n",
    "    for n_masks in mask_sizes:\n",
    "        sampled_pert_cells = pert_cells.sample(n=n_masks, replace=False)\n",
    "        for dim in range(10):\n",
    "            dimension_values[dim].append(sampled_pert_cells.mean().values[dim])\n",
    "\n",
    "    # Plot the first 10 dimensions as subplots\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "    fig.suptitle('First 10 Dimensions vs. Number of Masks')\n",
    "\n",
    "    for dim in range(10):\n",
    "        ax = axes[dim // 2, dim % 2]\n",
    "        ax.plot(mask_sizes, dimension_values[dim], marker='o')\n",
    "        ax.set_xlabel('Number of Masks')\n",
    "        ax.set_ylabel(f'Dimension {dim + 1} Value')\n",
    "        ax.grid()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "#analyze_dimension_fluctuations(model, adata_combined, batch=128, fraction=0.15, step_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the mask size is optimised, it is time to run the search for the optimal Fibroblast - iPSCs reprogramming factors!\n",
    "We start by iterating over 3-gene subsets and measuring correlation with all the available reprogramming pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate perturbed cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loader': <torch_geometric.deprecation.DataLoader at 0x7fc6c85c99c0>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data = PertData(\"../data/\")\n",
    "pert_data.load(data_path='../data/fibroblast_fm6_with_fm1_raw/') #'../data/fibroblast_fm6_with_fm1_raw/'\n",
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_data.adata.X = pert_data.adata.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - WARNING - The input data seems to be already log1p transformed. Set `log1p=False` to avoid double log1p transform.\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor(pert_data.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OKSM = ['SOX2', 'KLF4', 'POU5F1', 'MYC', 'NANOG']\n",
    "\n",
    "# TFs = pd.read_csv('../../perturb_train/little_data/TF_db.csv', index_col = 0)\n",
    "# TFs = TFs.loc[:,'HGNC symbol'].tolist()\n",
    "\n",
    "# ipsc_genes = pert_data.adata.var.gene_name.tolist()\n",
    "# to_perturb = list(set(ipsc_genes).intersection(set(TFs)))\n",
    "\n",
    "# tf_exp = pert_data.adata[:,pert_data.adata.var.gene_name.isin(to_perturb)].X\n",
    "\n",
    "# a = (tf_exp.shape[0] - (tf_exp==0).sum(axis=0))/tf_exp.shape[0]\n",
    "# threshold = 0.10  # Set your desired threshold here\n",
    "\n",
    "# # Get the indexes where values are above the threshold\n",
    "# to_perturb = np.array(to_perturb)[np.array(a).squeeze() > threshold]\n",
    "# to_perturb = np.array(list(set(list(to_perturb) + OKSM)))\n",
    "\n",
    "# len(to_perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'SOX2' in to_perturb, 'KLF4' in to_perturb, 'MYC' in to_perturb, 'POU5F1'in to_perturb, 'NANOG' in to_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 random 3-element subsets from 981 elements\n",
      "\n",
      "Total subsets generated: 50000\n",
      "\n",
      "First 5 subsets:\n",
      "Subset 1: ('BCLAF1', 'HDAC6', 'ZNF540')\n",
      "Subset 2: ('ETV3', 'NRK', 'ZFP1')\n",
      "Subset 3: ('SMAD7', 'OGG1', 'ZNF438')\n",
      "Subset 4: ('DNMT3B', 'ZNF408', 'EIF3C')\n",
      "Subset 5: ('KAT6A', 'TBX20', 'CEBPD')\n",
      "\n",
      "Last 5 subsets:\n",
      "Subset 49996: ('MKX', 'ZNF483', 'ZDHHC11')\n",
      "Subset 49997: ('MEIS3', 'JMY', 'ARHGAP35')\n",
      "Subset 49998: ('RFX4', 'DZIP1', 'ZNF790')\n",
      "Subset 49999: ('ZNF646', 'BCOR', 'ZKSCAN5')\n",
      "Subset 50000: ('REL', 'SERTAD2', 'DDX3X')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fallback implementation of comb for Python versions earlier than 3.8\n",
    "def comb(n, k):\n",
    "    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n",
    "\n",
    "def generate_random_subset(elements, k):\n",
    "    return tuple(sorted(random.sample(elements, k)))\n",
    "\n",
    "def generate_random_subsets(elements, k, count):\n",
    "    total_possible = comb(len(elements), k)\n",
    "    if count > total_possible:\n",
    "        raise ValueError(f\"Requested count ({count}) exceeds total possible combinations ({total_possible})\")\n",
    "    \n",
    "    seen = set()\n",
    "    subsets = []\n",
    "    \n",
    "    while len(subsets) < count:\n",
    "        subset = generate_random_subset(range(len(elements)), k)\n",
    "        if subset not in seen:\n",
    "            seen.add(subset)\n",
    "            subsets.append(tuple(elements[i] for i in subset))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def get_subsets(to_perturb, num_subsets=50_000, seed=42):\n",
    "    random.seed(seed)  # Set seed for reproducibility\n",
    "    subset_size = 3\n",
    "    \n",
    "    print(f\"Generating {num_subsets} random {subset_size}-element subsets from {len(to_perturb)} elements\")\n",
    "    return generate_random_subsets(to_perturb, subset_size, num_subsets)\n",
    "\n",
    "subsets = get_subsets(to_perturb)\n",
    "\n",
    "print(f\"\\nTotal subsets generated: {len(subsets)}\")\n",
    "print(\"\\nFirst 5 subsets:\")\n",
    "for i, subset in enumerate(subsets[:5]):\n",
    "    print(f\"Subset {i+1}: {subset}\")\n",
    "\n",
    "print(f\"\\nLast 5 subsets:\")\n",
    "for i, subset in enumerate(subsets[-5:]):\n",
    "    print(f\"Subset {len(subsets)-4+i}: {subset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loader': <torch_geometric.deprecation.DataLoader at 0x7fc7e1f57160>}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adata_list = []\n",
    "# for sample in ['d20-nr', 'p20-nr', 'd8-fm', 'd4-fm', 'p3-nr']:\n",
    "#     pert_data.load(data_path = f'../data/fibroblast_{sample}')\n",
    "#     adata_list.append(pert_data.adata)\n",
    "\n",
    "# import anndata as ad    \n",
    "# adata_combined = ad.concat(adata_list, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Generate all 3-element subsets\n",
    "subsets_oksm = list(combinations(OKSM, 4))\n",
    "\n",
    "best_model = model\n",
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_gene_subset(expression, adata_var_index, model, gene_names_to_perturb, batch=4, n_masks=512, fraction=0.3):\n",
    "    \"\"\"\n",
    "    Perturb a desired subset of genes provided as a list of gene names.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing gene expression data\n",
    "    - model: PyTorch model used to sample perturbations\n",
    "    - gene_names_to_perturb: list of strings, names of genes to be perturbed\n",
    "    - batch: int, batch size for sampling perturbations\n",
    "    - n_masks: int, number of binary masks to generate for sampling\n",
    "    - fraction: float, fraction of genes to perturb in each mask\n",
    "\n",
    "    Returns:\n",
    "    - ref_cells: DataFrame, reference cell reconstructions\n",
    "    - pert_cells: DataFrame, perturbed cell reconstructions\n",
    "    \"\"\"\n",
    "    # Extract expression data\n",
    "    \n",
    "    expression_pert = expression.copy()\n",
    "    \n",
    "    # Find indices of genes to perturb\n",
    "    gene_indices_to_perturb = [adata_var_index.get_loc(gene) for gene in gene_names_to_perturb]\n",
    "    \n",
    "    # Apply overexpression to the selected genes\n",
    "    expression_pert[:, gene_indices_to_perturb] = expression_pert.max() * 1\n",
    "    \n",
    "    # Convert to tensors\n",
    "    #single_cell_ref = torch.tensor(expression)\n",
    "    single_cell_pert = torch.tensor(expression_pert)\n",
    "    \n",
    "    # Generate binary masks\n",
    "    n_genes = expression_pert.shape[1]\n",
    "    masks = generate_binary_array(n_masks, n_genes, fraction)\n",
    "    \n",
    "    # Sample reference cells\n",
    "    # ref_cells = []\n",
    "    # for i in range(0, masks.shape[0] - batch, batch):\n",
    "    #     res = sample_pert(model, Data(x=single_cell_ref), masks[i:i + batch]).detach().cpu().numpy()\n",
    "    #     ref_cells.append(res)\n",
    "    \n",
    "    # Sample perturbed cells\n",
    "    pert_cells = []\n",
    "    for i in range(0, masks.shape[0] - batch, batch):\n",
    "        res = sample_pert(model, Data(x=single_cell_pert), masks[i:i + batch]).detach().cpu().numpy()\n",
    "        pert_cells.append(res)\n",
    "    \n",
    "    # Convert results to DataFrames\n",
    "    # ref_cells = np.array(ref_cells).reshape(-1, n_genes)\n",
    "    # ref_cells = pd.DataFrame(ref_cells, columns=adata.var.index.tolist())\n",
    "\n",
    "    pert_cells = np.array(pert_cells).reshape(-1, n_genes)\n",
    "    pert_cells = pd.DataFrame(pert_cells, columns=adata_combined.var.index.tolist())\n",
    "    \n",
    "    return pert_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fm1', 'fm6', 'mix', 'pr2', 'nr3', 'nic']\n",
       "Categories (6, object): ['fm1', 'fm6', 'mix', 'nic', 'nr3', 'pr2']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_combined.obs.cluster.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4490"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(adata_combined.X[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_c = sc.read_h5ad('../data/readySeu_hrpi_GOOD.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = pd.read_csv('../data/gene_info.csv',index_col=0)\n",
    "mapa = mapa.set_index('feature_id')\n",
    "\n",
    "def f(x):\n",
    "    try: \n",
    "        return mapa.loc[x]['feature_name']\n",
    "    except:\n",
    "        return -1\n",
    "        \n",
    "adata_c.var['names'] = adata_c.var.name.apply(lambda x: f(x))\n",
    "adata_c = adata_c[:,adata_c.var.names!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_c.var = adata_c.var.set_index('names')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_c.X = adata_c.X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_c = adata_c[:,to_perturb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessor = Preprocessor(\n",
    "#     use_key=\"X\",  # the key in adata.layers to use as raw data\n",
    "#     filter_gene_by_counts=1,  # step 1\n",
    "#     filter_cell_by_counts=False,  # step 2\n",
    "#     normalize_total=False,  # 3. whether to normalize the raw data and to what sum\n",
    "#     result_normed_key=\"X_normed\",  # the key in adata.layers to store the normalized data\n",
    "#     log1p=False,  # 4. whether to log1p the normalized data\n",
    "#     result_log1p_key=\"X_log1p\",\n",
    "#     subset_hvg=n_hvg,  # 5. whether to subset the raw data to highly variable genes\n",
    "#     hvg_flavor=\"seurat_v3\" if data_is_raw else \"cell_ranger\",\n",
    "#     binning=n_bins,  # 6. whether to bin the raw data and to what number of bins\n",
    "#     result_binned_key=\"X\",  # the key in adata.layers to store the binned data\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Filtering genes by counts ...\n",
      "scGPT - INFO - Normalizing total counts ...\n",
      "scGPT - INFO - Log1p transforming ...\n",
      "scGPT - WARNING - The input data seems to be already log1p transformed. Set `log1p=False` to avoid double log1p transform.\n",
      "scGPT - INFO - Binning data ...\n"
     ]
    }
   ],
   "source": [
    "preprocessor(adata_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_combined = adata_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_combined.X = adata_combined.layers['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This was an attempt to feed Norman data into the model - same bad result\n",
    "# tempy = pert_data.adata[pert_data.adata.obs.condition=='ctrl',pert_data.adata.var.gene_name.isin(to_perturb)].copy()\n",
    "# tempy.X = tempy.layers['X']\n",
    "# tempy = tempy.X[0,:]\n",
    "# tempy = np.array(tempy.tolist() + [0.0] * (adata_combined.X.shape[1]-tempy.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the empty perturbation-reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cell_n = 3001\n",
    "reconstructed_ref = True\n",
    "iterations = 1  # Number of random sampling iterations\n",
    "n_masks = 128\n",
    "batch_size = min(128, n_masks-1)\n",
    "fraction_masked = 0.0\n",
    "#t = adata_combined[adata_combined.obs.condition=='ctrl'].copy()\n",
    "adata_var_index = adata_combined.var.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:02<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "libs = {}\n",
    "\n",
    "for library in ['fm1']:# ['fm1', 'fm6', 'mix', 'pr2', 'nr3', 'nic']:\n",
    "    if reconstructed_ref:\n",
    "        target = adata_combined[adata_combined.obs.cluster==library].copy()\n",
    "        ref_cells_fin_list = []\n",
    "        for i in tqdm(range(1)):\n",
    "            choice = i #np.random.randint(low=0, high=t.X.shape[0])\n",
    "            expression = target.X[choice,:]  #.toarray()\n",
    "            \n",
    "            single_cell_ref = torch.tensor(expression)\n",
    "            n_genes = single_cell_ref.reshape(1,-1).shape[1]\n",
    "            masks = generate_binary_array(n_masks, n_genes, fraction_masked)\n",
    "            \n",
    "            ref_cells = []\n",
    "            for i in range(0, masks.shape[0]-batch_size, batch_size):\n",
    "                res = sample_pert(model, Data(x=single_cell_ref), masks[i:i+batch_size],).detach().cpu().numpy()\n",
    "                ref_cells.append(res)\n",
    "                \n",
    "            ref_cells = np.array(ref_cells).reshape(-1, len(to_perturb))\n",
    "            ref_cells = pd.DataFrame(ref_cells, columns = adata_combined.var.index.tolist())\n",
    "            ref_cells_fin_list.append(ref_cells.mean())\n",
    "        libs[library] = np.mean(ref_cells_fin_list,axis=0)\n",
    "    else:\n",
    "        ref_cells = adata_combined[adata_combined.obs.library==library].X\n",
    "        libs[library] = ref_cells.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(ref_cells.mean(), expression)\n",
    "plt.xlabel('Reconstructed Gene Expressions')\n",
    "plt.ylabel('Actual Gene Expressions (norm+log+bin)')\n",
    "plt.savefig('./recon_fibroblast.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6/6 [00:06<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "res = pd.DataFrame()\n",
    "counter = 0\n",
    "\n",
    "for s in tqdm(subsets_oksm+[OKSM]):\n",
    "    genes = [s]\n",
    "    name = '_'.join(genes[0])\n",
    "    for i in range(iterations):        \n",
    "        expression = t.X[cell_n,:].toarray()\n",
    "    \n",
    "        single_cell = perturb_gene_subset(expression, adata_var_index, model, s, batch=batch_size, n_masks=n_masks, fraction=fraction_masked).values\n",
    "        \n",
    "        for lib in libs:\n",
    "            res.loc[f'{name}_{choice}', f'euclid_distance_{lib}'] = np.mean([np.linalg.norm(libs[lib] - single_cell[i]) for i in range(single_cell.shape[0])])\n",
    "            res.loc[f'{name}_{choice}', f'spearman_corr_{lib}'] = np.mean([stats.spearmanr(libs[lib], single_cell[i])[0] for i in range(single_cell.shape[0])])\n",
    "        res.to_csv(f'./save/ipsc_perturbation_sampling_search_oksm_1024_masks_RECON_REF_multi_cells_v2_cell_{cell_n}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|         | 5042/50000 [1:29:49<13:20:58,  1.07s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame()\n",
    "choice = cell_n #np.random.randint(low=0, high=t.X.shape[0])\n",
    "expression = t.X[choice, :].toarray()\n",
    "\n",
    "# Perturbation Search\n",
    "if 1 == 1:\n",
    "    for s in tqdm(subsets):\n",
    "        genes = [s]\n",
    "        name = '_'.join(genes[0])\n",
    "        mean_euclid_distance = {lib: [] for lib in libs}\n",
    "        mean_pearson_corr = {lib: [] for lib in libs}\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            single_cell = perturb_gene_subset(expression, adata_var_index, model, s, batch=batch_size, n_masks=n_masks, fraction=fraction_masked).values\n",
    "            for lib in libs:\n",
    "                euclid_distances = [np.linalg.norm(libs[lib] - single_cell[i]) for i in range(single_cell.shape[0])]\n",
    "                pearson_corrs = [stats.spearmanr(libs[lib], single_cell[i])[0] for i in range(single_cell.shape[0])] #[np.corrcoef(libs[lib], single_cell[i])[0, 1] for i in range(single_cell.shape[0])]\n",
    "                res.loc[name, f'euclid_distance_{lib}'] = np.mean(euclid_distances)\n",
    "                res.loc[name, f'spearman_corr_{lib}'] = np.mean(pearson_corrs)\n",
    "\n",
    "        counter += 1\n",
    "        clear_output()\n",
    "        if counter >= 100:\n",
    "            res.to_csv(f'./save/ipsc_perturbation_sampling_search_1024_masks_RECON_REF_multi_cells_v2_cell_{cell_n}.csv')\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real data relationshiops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = pd.DataFrame(index = libs.keys(), columns = libs.keys())\n",
    "for lib1 in libs:\n",
    "    for lib2 in libs:\n",
    "        pearson_corrs = np.corrcoef(libs[lib1], libs[lib2])[0, 1]\n",
    "        corrs.loc[lib1,lib2] = pearson_corrs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = ['D0-fm','D4-fm','D8-fm','D20-nr','P3-nr','P20-nr']\n",
    "sns.heatmap(corrs.loc[order, order].astype(float))\n",
    "plt.savefig('./save/reprog_corr_heatmap.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libs[lib1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
