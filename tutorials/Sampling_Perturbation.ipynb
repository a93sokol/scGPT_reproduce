{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a pre-trained model and extracting information about context specific feature impacts using Sampling Perturbation method for Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpcfs/users/a1234104/miniconda3/envs/scGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext._torchtext import (\n",
    "    Vocab as VocabPybind,\n",
    ")\n",
    "from torch_geometric.loader import DataLoader\n",
    "from gears import PertData, GEARS\n",
    "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
    "from gears.utils import create_cell_graph_dataset_for_prediction\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import scgpt as scg\n",
    "from scgpt.model import TransformerGenerator\n",
    "from scgpt.loss import (\n",
    "    masked_mse_loss,\n",
    "    criterion_neg_log_bernoulli,\n",
    "    masked_relative_error,\n",
    ")\n",
    "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
    "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
    "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
    "\n",
    "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for data prcocessing\n",
    "pad_token = \"<pad>\"\n",
    "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
    "pad_value = 0  # for padding values\n",
    "pert_pad_id = 0\n",
    "include_zero_gene = \"all\"\n",
    "max_seq_len = 1536\n",
    "\n",
    "# settings for training\n",
    "MLM = True  # whether to use masked language modeling, currently it is always on.\n",
    "CLS = False  # celltype classification objective\n",
    "CCE = False  # Contrastive cell embedding objective\n",
    "MVC = False  # Masked value prediction for cell embedding\n",
    "ECS = False  # Elastic cell similarity objective\n",
    "amp = True\n",
    "load_model = \"../save/scGPT_human\"\n",
    "load_param_prefixs = [\n",
    "    \"encoder\",\n",
    "    \"value_encoder\",\n",
    "    \"transformer_encoder\",\n",
    "]\n",
    "\n",
    "# settings for optimizer\n",
    "lr = 1e-4  # or 1e-4\n",
    "batch_size = 64\n",
    "eval_batch_size = 64\n",
    "epochs = 3\n",
    "schedule_interval = 1\n",
    "early_stop = 10\n",
    "\n",
    "# settings for the model\n",
    "embsize = 512  # embedding dimension\n",
    "d_hid = 512  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 12  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8  # number of heads in nn.MultiheadAttention\n",
    "n_layers_cls = 3\n",
    "dropout = 0  # dropout probability\n",
    "use_fast_transformer = True  # whether to use fast transformer\n",
    "\n",
    "# logging\n",
    "log_interval = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loader': <torch_geometric.deprecation.DataLoader at 0x7fb2133b3700>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data = PertData(\"../data/\")\n",
    "pert_data.load(data_path='../data/fibroblast_p20-nr/')\n",
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to save/dev_perturb_testing_sampling_pert-Oct17-09-46\n",
      "scGPT - INFO - Running on 2024-10-17 09:46:05\n"
     ]
    }
   ],
   "source": [
    "data_name = 'testing_sampling_pert'\n",
    "save_dir = Path(f\"./save/dev_perturb_{data_name}-{time.strftime('%b%d-%H-%M')}/\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"saving to {save_dir}\")\n",
    "\n",
    "logger = scg.logger\n",
    "scg.utils.add_file_handler(logger, save_dir / \"run.log\")\n",
    "# log running date and current git commit\n",
    "logger.info(f\"Running on {time.strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - match 17315/17315 genes in vocabulary of size 60697.\n",
      "scGPT - INFO - Resume model from ../save/scGPT_human/best_model.pt, the model args will override the config ../save/scGPT_human/args.json.\n"
     ]
    }
   ],
   "source": [
    "if load_model is not None:\n",
    "    model_dir = Path(load_model)\n",
    "    model_config_file = model_dir / \"args.json\"\n",
    "    model_file = model_dir / \"best_model.pt\"\n",
    "    vocab_file = model_dir / \"vocab.json\"\n",
    "\n",
    "    vocab = GeneVocab.from_file(vocab_file)\n",
    "    for s in special_tokens:\n",
    "        if s not in vocab:\n",
    "            vocab.append_token(s)\n",
    "\n",
    "    pert_data.adata.var[\"id_in_vocab\"] = [\n",
    "        1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
    "    ]\n",
    "    gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
    "    logger.info(\n",
    "        f\"match {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)} genes \"\n",
    "        f\"in vocabulary of size {len(vocab)}.\"\n",
    "    )\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "\n",
    "    # model\n",
    "    with open(model_config_file, \"r\") as f:\n",
    "        model_configs = json.load(f)\n",
    "    logger.info(\n",
    "        f\"Resume model from {model_file}, the model args will override the \"\n",
    "        f\"config {model_config_file}.\"\n",
    "    )\n",
    "    embsize = model_configs[\"embsize\"]\n",
    "    nhead = model_configs[\"nheads\"]\n",
    "    d_hid = model_configs[\"d_hid\"]\n",
    "    nlayers = model_configs[\"nlayers\"]\n",
    "    n_layers_cls = model_configs[\"n_layers_cls\"]\n",
    "else:\n",
    "    genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
    "    vocab = Vocab(\n",
    "        VocabPybind(genes + special_tokens, None)\n",
    "    )  # bidirectional lookup [gene <-> int]\n",
    "vocab.set_default_index(vocab[\"<pad>\"])\n",
    "gene_ids = np.array(\n",
    "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
    ")\n",
    "n_genes = len(genes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Create and train scGpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scgpt.model import TransformerModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scGPT - INFO - Loading params encoder.embedding.weight with shape torch.Size([60697, 512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params encoder.enc_norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.weight with shape torch.Size([512, 1])\n",
      "scGPT - INFO - Loading params value_encoder.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params value_encoder.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params value_encoder.norm.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.0.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.1.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.2.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.3.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.4.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.5.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.6.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.7.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.8.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.9.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.10.norm2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.weight with shape torch.Size([1536, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.Wqkv.bias with shape torch.Size([1536])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.self_attn.out_proj.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.weight with shape torch.Size([512, 512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.linear2.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm1.bias with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.weight with shape torch.Size([512])\n",
      "scGPT - INFO - Loading params transformer_encoder.layers.11.norm2.bias with shape torch.Size([512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (encoder): GeneEncoder(\n",
       "    (embedding): Embedding(60697, 512, padding_idx=60694)\n",
       "    (enc_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (value_encoder): ContinuousValueEncoder(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "    (linear1): Linear(in_features=1, out_features=512, bias=True)\n",
       "    (activation): ReLU()\n",
       "    (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (8): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (9): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (10): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (11): FlashTransformerEncoderLayer(\n",
       "        (self_attn): FlashMHA(\n",
       "          (Wqkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (inner_attn): FlashAttention()\n",
       "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0, inplace=False)\n",
       "        (dropout2): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): ExprDecoder(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "      (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (cls_decoder): ClsDecoder(\n",
       "    (_decoder): ModuleList(\n",
       "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (4): ReLU()\n",
       "      (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (out_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (sim): Similarity(\n",
       "    (cos): CosineSimilarity()\n",
       "  )\n",
       "  (creterion_cce): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "model = TransformerModel(\n",
    "    ntokens,\n",
    "    embsize,\n",
    "    nhead,\n",
    "    d_hid,\n",
    "    nlayers,\n",
    "    nlayers_cls=n_layers_cls,\n",
    "    n_cls=1,\n",
    "    vocab=vocab,\n",
    "    dropout=dropout,\n",
    "    pad_token=pad_token,\n",
    "    pad_value=pad_value,\n",
    "    use_fast_transformer=use_fast_transformer,\n",
    ")\n",
    "if load_param_prefixs is not None and load_model is not None:\n",
    "    # only load params that start with the prefix\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = torch.load(model_file,  map_location=torch.device(device))\n",
    "    pretrained_dict = {\n",
    "        k: v\n",
    "        for k, v in pretrained_dict.items()\n",
    "        if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
    "    }\n",
    "    for k, v in pretrained_dict.items():\n",
    "        logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "elif load_model is not None:\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(model_file))\n",
    "        logger.info(f\"Loading all model params from {model_file}\")\n",
    "    except:\n",
    "        # only load params that are in the model and match the size\n",
    "        model_dict = model.state_dict()\n",
    "        pretrained_dict = torch.load(model_file, map_location=torch.device(device))\n",
    "        pretrained_dict = {\n",
    "            k: v\n",
    "            for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and v.shape == model_dict[k].shape\n",
    "        }\n",
    "        for k, v in pretrained_dict.items():\n",
    "            logger.info(f\"Loading params {k} with shape {v.shape}\")\n",
    "        model_dict.update(pretrained_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_array(n_masks, n_genes, fraction):\n",
    "    \"\"\"\n",
    "    Generate a binary 2D NumPy array with a set fraction of 1s randomly distributed in every row.\n",
    "\n",
    "    Parameters:\n",
    "    rows (int): Number of rows in the array.\n",
    "    cols (int): Number of columns in the array.\n",
    "    fraction (float): Fraction of 1s in each row (0 <= fraction <= 1).\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Binary 2D NumPy array.\n",
    "    \"\"\"\n",
    "    if not (0 <= fraction <= 1):\n",
    "        raise ValueError(\"Fraction must be between 0 and 1.\")\n",
    "\n",
    "    array = np.zeros((n_masks, n_genes), dtype=int)\n",
    "    num_ones = int(fraction * n_genes)\n",
    "\n",
    "    for row in array:\n",
    "        ones_indices = np.random.choice(n_genes, num_ones, replace=False)\n",
    "        row[ones_indices] = 1\n",
    "\n",
    "    return array\n",
    "\n",
    "def apply_masks(\n",
    "    values: Union[torch.Tensor, np.ndarray],\n",
    "    masks: np.ndarray,\n",
    "    mask_value: int = -1,\n",
    "    pad_value: int = 0,\n",
    "    indices_to_keep: List[int] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply given masks to the values.\n",
    "\n",
    "    Args:\n",
    "        values (array-like): The data to mask, shape (n_features,)\n",
    "        masks (array-like): An array of masks to apply, shape (n_masks, n_features)\n",
    "        mask_value (int): The value to mask with, default to -1.\n",
    "        pad_value (int): The value of padding in the values, will be kept unchanged.\n",
    "        indices_to_keep (list of int): List of indices that should not be masked.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of masked data, shape (n_masks, n_features)\n",
    "    \"\"\"\n",
    "    if isinstance(values, torch.Tensor):\n",
    "        values = values.clone().detach().cpu().numpy()\n",
    "    else:\n",
    "        values = values.copy()\n",
    "\n",
    "    if indices_to_keep is None:\n",
    "        indices_to_keep = []\n",
    "\n",
    "    masked_values = []\n",
    "    for i in range(masks.shape[0]):\n",
    "        mask = masks[i]\n",
    "        masked_value = values.copy()\n",
    "        # Only mask the positions where mask == 1 and values != pad_value\n",
    "        mask_positions = (mask == 1) & (values != pad_value)\n",
    "        # Set the indices that should not be masked to False\n",
    "        mask_positions[indices_to_keep] = False\n",
    "        masked_value[mask_positions] = mask_value\n",
    "        masked_values.append(masked_value)\n",
    "\n",
    "    return torch.from_numpy(np.array(masked_values)).float()\n",
    "\n",
    "def sample_pert(\n",
    "    model: nn.Module,\n",
    "    cell_data: torch_geometric.data.Data,\n",
    "    masks: np.ndarray,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies multiple masks to a single cell record, reconstructs the missing values using the model,\n",
    "    and returns the reconstructed cell expressions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The transformer model.\n",
    "        cell_data (torch_geometric.data.Data): The data of a single cell.\n",
    "        masks (np.ndarray): An array of masks to apply, shape (n_masks, n_genes).\n",
    "        device (torch.device): The device to run the computations on.\n",
    "        map_raw_id_to_vocab_id (Callable): Function to map raw gene IDs to vocab IDs.\n",
    "        gene_ids (torch.Tensor): Tensor of gene IDs.\n",
    "        criterion (Callable): Loss function.\n",
    "        amp (bool): Automatic Mixed Precision flag.\n",
    "        CLS, CCE, MVC, ECS: Model-specific flags.\n",
    "        vocab (Dict): Vocabulary mapping.\n",
    "        pad_token (str): Padding token.\n",
    "        max_seq_len (int, optional): Maximum sequence length.\n",
    "        include_zero_gene (str): Inclusion criteria for zero-expression genes.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Reconstructed cell expressions, shape (n_masks, n_genes).\n",
    "    \"\"\"\n",
    "    #initial_state_dict = model.state_dict()\n",
    "    \n",
    "    model.eval()\n",
    "    cell_data.to(device)\n",
    "    x: torch.Tensor = cell_data.x  # (n_genes, 2)\n",
    "    ori_gene_values = x[0, :]  # (n_genes,)\n",
    "    pert_flags = x[:, 1].long()  # (n_genes,)\n",
    "    target_gene_values = cell_data.x  # (n_genes,)\n",
    "\n",
    "    n_masks = masks.shape[0]\n",
    "    n_genes = target_gene_values.shape[1]\n",
    "\n",
    "    # Repeat ori_gene_values and pert_flags for each mask\n",
    "    \n",
    "    #ori_gene_values = ori_gene_values.unsqueeze(0).repeat(n_masks, 1)  # (n_masks, n_genes)\n",
    "        \n",
    "    # Prepare input_gene_ids\n",
    "    if include_zero_gene in [\"all\", \"batch-wise\"]:\n",
    "        if include_zero_gene == \"all\":\n",
    "            input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
    "        else:\n",
    "            input_gene_ids = (\n",
    "                ori_gene_values.nonzero()[:, 1].flatten().unique().sort()[0]\n",
    "            )\n",
    "        # Sample input_gene_id\n",
    "        if max_seq_len and len(input_gene_ids) > max_seq_len:\n",
    "            # input_gene_ids = torch.randperm(len(input_gene_ids), device=device)[\n",
    "            #     :max_seq_len\n",
    "            # ]\n",
    "            input_gene_ids = torch.tensor(np.arange(max_seq_len)).to(device)\n",
    "\n",
    "        \n",
    "        input_values = ori_gene_values[input_gene_ids]\n",
    "    \n",
    "        input_values = apply_masks(\n",
    "                                        input_values,\n",
    "                                        masks[:, input_gene_ids.cpu().numpy()],\n",
    "                                        mask_value=-1,\n",
    "                                        pad_value=0,\n",
    "                                    ) \n",
    "    \n",
    "        \n",
    "        mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
    "        mapped_input_gene_ids = mapped_input_gene_ids.unsqueeze(0).repeat(n_masks, 1)\n",
    "\n",
    "        # src_key_padding_mask\n",
    "        src_key_padding_mask = mapped_input_gene_ids.eq(vocab[pad_token])\n",
    "    \n",
    "    \n",
    "    with torch.cuda.amp.autocast(enabled=amp):\n",
    "        output_dict = model(\n",
    "            mapped_input_gene_ids.to(device),\n",
    "            input_values.to(device),\n",
    "            src_key_padding_mask=src_key_padding_mask.to(device),\n",
    "            CLS=CLS,\n",
    "            CCE=CCE,\n",
    "            MVC=MVC,\n",
    "            ECS=ECS,\n",
    "        )\n",
    "        output_values = output_dict[\"mlm_output\"]  # (n_masks, n_genes)\n",
    "    \n",
    "    # Reset model to initial state\n",
    "    # model.load_state_dict(initial_state_dict)\n",
    "    return output_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the reconstructed reprogramming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKSM = ['SOX2', 'KLF4', 'POU5F1', 'MYC', 'NANOG']\n",
    "\n",
    "TFs = pd.read_csv('../../perturb_train/little_data/TF_db.csv', index_col = 0)\n",
    "TFs = TFs.loc[:,'HGNC symbol'].tolist()\n",
    "\n",
    "ipsc_genes = pert_data.adata.var.gene_name.tolist()\n",
    "to_perturb = list(set(ipsc_genes).intersection(set(TFs)))\n",
    "\n",
    "tf_exp = pert_data.adata[:,pert_data.adata.var.gene_name.isin(to_perturb)].X\n",
    "\n",
    "a = (tf_exp.shape[0] - (tf_exp==0).sum(axis=0))/tf_exp.shape[0]\n",
    "threshold = 0.10  # Set your desired threshold here\n",
    "\n",
    "# Get the indexes where values are above the threshold\n",
    "to_perturb = np.array(to_perturb)[np.array(a).squeeze() > threshold]\n",
    "to_perturb = np.array(list(set(list(to_perturb) + OKSM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    }
   ],
   "source": [
    "adata_list = []\n",
    "for sample in ['d20-nr', 'p20-nr', 'd8-fm', 'd4-fm', 'p3-nr']:\n",
    "    pert_data.load(data_path = f'../data/fibroblast_{sample}')\n",
    "    adata_list.append(pert_data.adata)\n",
    "\n",
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size,)\n",
    "import anndata as ad    \n",
    "adata_combined = ad.concat(adata_list, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_combined = adata_combined[:,adata_combined.var.index.isin(to_perturb)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train_loader = pert_data.dataloader[\"train_loader\"]\n",
    "# n_masks = 12500\n",
    "# batch = 128\n",
    "# fraction = 0.15\n",
    "# libs = {}\n",
    "# for library in ['D0-fm', 'D20-nr', 'P20-nr', 'D8-fm', 'D4-fm', 'P3-nr']:\n",
    "#     expression = adata_combined[adata_combined.obs.library==library].X[0].toarray()\n",
    "#     single_cell_ref = torch.tensor(expression)\n",
    "#     n_genes = single_cell_ref.shape[1]\n",
    "#     masks = generate_binary_array(n_masks, n_genes, fraction)\n",
    "    \n",
    "#     ref_cells = []\n",
    "#     for i in range(0, masks.shape[0]-batch, batch):\n",
    "#         res = sample_pert(model, Data(x=single_cell_ref), masks[i:i+batch]).detach().cpu().numpy()\n",
    "#         ref_cells.append(res)\n",
    "        \n",
    "#     ref_cells = np.array(ref_cells).reshape(-1, len(to_perturb))\n",
    "#     ref_cells = pd.DataFrame(ref_cells, columns = adata_combined.var.index.tolist())\n",
    "#     libs[library] = ref_cells.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise the number of masks needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dimension_fluctuations(model, adata, batch=4, fraction=0.3, step_size=64):\n",
    "    \"\"\"\n",
    "    Analyze how the first 10 dimensions fluctuate with increasing mask sizes.\n",
    "\n",
    "    Parameters:\n",
    "    - single_cell_pert: numpy array, perturbed cell data\n",
    "    - model: PyTorch model used to sample perturbations\n",
    "    - adata: AnnData object containing experimental data\n",
    "    - batch: int, batch size for sampling perturbations\n",
    "    - fraction: float, fraction of genes to perturb\n",
    "    - step_size: int, increment in number of masks for each iteration\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    n_genes = adata.var.shape[0]\n",
    "    single_cell_pert = adata.X[0].toarray()\n",
    "    \n",
    "    max_masks = 100000\n",
    "    mask_sizes = list(range(step_size, max_masks + 1, step_size))\n",
    "\n",
    "    # Initialize a list to hold dimension values for each mask size\n",
    "    dimension_values = [[] for _ in range(10)]\n",
    "\n",
    "    pert = adata[adata.obs.condition != 'ctrl'].X.toarray()\n",
    "\n",
    "    # Generate the maximum number of masks once\n",
    "    masks = generate_binary_array(max_masks, n_genes, fraction)\n",
    "\n",
    "    # Sample perturbed cells using the maximum number of masks\n",
    "    pert_cells = []\n",
    "    for i in range(0, masks.shape[0] - batch, batch):\n",
    "        res_pert = sample_pert(model, Data(x=torch.tensor(single_cell_pert)), masks[i:i + batch]).detach().cpu().numpy()\n",
    "        pert_cells.append(res_pert)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    pert_cells = np.array(pert_cells).reshape(-1, single_cell_pert.shape[1])\n",
    "    pert_cells = pd.DataFrame(pert_cells, columns=adata.var.index.tolist())\n",
    "\n",
    "    # Calculate and store the mean values of the first 10 dimensions for each mask size\n",
    "    for n_masks in mask_sizes:\n",
    "        sampled_pert_cells = pert_cells.sample(n=n_masks, replace=False)\n",
    "        for dim in range(10):\n",
    "            dimension_values[dim].append(sampled_pert_cells.mean().values[dim])\n",
    "\n",
    "    # Plot the first 10 dimensions as subplots\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(15, 20))\n",
    "    fig.suptitle('First 10 Dimensions vs. Number of Masks')\n",
    "\n",
    "    for dim in range(10):\n",
    "        ax = axes[dim // 2, dim % 2]\n",
    "        ax.plot(mask_sizes, dimension_values[dim], marker='o')\n",
    "        ax.set_xlabel('Number of Masks')\n",
    "        ax.set_ylabel(f'Dimension {dim + 1} Value')\n",
    "        ax.grid()\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "    plt.show()\n",
    "\n",
    "#analyze_dimension_fluctuations(model, adata_combined, batch=128, fraction=0.15, step_size=4096)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After the mask size is optimised, it is time to run the search for the optimal Fibroblast - iPSCs reprogramming factors!\n",
    "We start by iterating over 3-gene subsets and measuring correlation with all the available reprogramming pathways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate perturbed cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loader': <torch_geometric.deprecation.DataLoader at 0x7fb21245d780>}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data = PertData(\"../data/\")\n",
    "pert_data.load(data_path='../data/fibroblast_p20-nr/')\n",
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "OKSM = ['SOX2', 'KLF4', 'POU5F1', 'MYC', 'NANOG']\n",
    "\n",
    "TFs = pd.read_csv('../../perturb_train/little_data/TF_db.csv', index_col = 0)\n",
    "TFs = TFs.loc[:,'HGNC symbol'].tolist()\n",
    "\n",
    "ipsc_genes = pert_data.adata.var.gene_name.tolist()\n",
    "to_perturb = list(set(ipsc_genes).intersection(set(TFs)))\n",
    "\n",
    "tf_exp = pert_data.adata[:,pert_data.adata.var.gene_name.isin(to_perturb)].X\n",
    "\n",
    "a = (tf_exp.shape[0] - (tf_exp==0).sum(axis=0))/tf_exp.shape[0]\n",
    "threshold = 0.10  # Set your desired threshold here\n",
    "\n",
    "# Get the indexes where values are above the threshold\n",
    "to_perturb = np.array(to_perturb)[np.array(a).squeeze() > threshold]\n",
    "to_perturb = np.array(list(set(list(to_perturb) + OKSM)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(to_perturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'SOX2' in to_perturb, 'KLF4' in to_perturb, 'MYC' in to_perturb, 'POU5F1'in to_perturb, 'NANOG' in to_perturb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 50000 random 3-element subsets from 1250 elements\n",
      "\n",
      "Total subsets generated: 50000\n",
      "\n",
      "First 5 subsets:\n",
      "Subset 1: ('LCORL', 'CXXC4', 'MIER3')\n",
      "Subset 2: ('ZNF730', 'KLF12', 'TRAF5')\n",
      "Subset 3: ('CYTL1', 'GPBP1', 'ZNF267')\n",
      "Subset 4: ('ZNF175', 'CTH', 'PRMT3')\n",
      "Subset 5: ('ZNF566', 'CC2D1B', 'HOXA11')\n",
      "\n",
      "Last 5 subsets:\n",
      "Subset 49996: ('GATA2', 'SMARCB1', 'ZNF20')\n",
      "Subset 49997: ('CARF', 'POU5F1B', 'HDAC10')\n",
      "Subset 49998: ('NME2', 'CUL5', 'CREB5')\n",
      "Subset 49999: ('ZSCAN10', 'MSX1', 'ZNF12')\n",
      "Subset 50000: ('MEOX2', 'DOT1L', 'SLC2A4RG')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Fallback implementation of comb for Python versions earlier than 3.8\n",
    "def comb(n, k):\n",
    "    return math.factorial(n) // (math.factorial(k) * math.factorial(n - k))\n",
    "\n",
    "def generate_random_subset(elements, k):\n",
    "    return tuple(sorted(random.sample(elements, k)))\n",
    "\n",
    "def generate_random_subsets(elements, k, count):\n",
    "    total_possible = comb(len(elements), k)\n",
    "    if count > total_possible:\n",
    "        raise ValueError(f\"Requested count ({count}) exceeds total possible combinations ({total_possible})\")\n",
    "    \n",
    "    seen = set()\n",
    "    subsets = []\n",
    "    \n",
    "    while len(subsets) < count:\n",
    "        subset = generate_random_subset(range(len(elements)), k)\n",
    "        if subset not in seen:\n",
    "            seen.add(subset)\n",
    "            subsets.append(tuple(elements[i] for i in subset))\n",
    "    \n",
    "    return subsets\n",
    "\n",
    "def get_subsets(to_perturb, num_subsets=50_000, seed=42):\n",
    "    random.seed(seed)  # Set seed for reproducibility\n",
    "    subset_size = 3\n",
    "    \n",
    "    print(f\"Generating {num_subsets} random {subset_size}-element subsets from {len(to_perturb)} elements\")\n",
    "    return generate_random_subsets(to_perturb, subset_size, num_subsets)\n",
    "\n",
    "subsets = get_subsets(to_perturb)\n",
    "\n",
    "print(f\"\\nTotal subsets generated: {len(subsets)}\")\n",
    "print(\"\\nFirst 5 subsets:\")\n",
    "for i, subset in enumerate(subsets[:5]):\n",
    "    print(f\"Subset {i+1}: {subset}\")\n",
    "\n",
    "print(f\"\\nLast 5 subsets:\")\n",
    "for i, subset in enumerate(subsets[-5:]):\n",
    "    print(f\"Subset {len(subsets)-4+i}: {subset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n",
      "Local copy of pyg dataset is detected. Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "adata_list = []\n",
    "for sample in ['d20-nr', 'p20-nr', 'd8-fm', 'd4-fm', 'p3-nr']:\n",
    "    pert_data.load(data_path = f'../data/fibroblast_{sample}')\n",
    "    adata_list.append(pert_data.adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local copy of split is detected. Loading...\n",
      "Done!\n",
      "Creating dataloaders....\n",
      "Dataloaders created...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loader': <torch_geometric.deprecation.DataLoader at 0x7fb2128a16f0>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_data.prepare_split(split='no_split', seed=1)\n",
    "pert_data.get_dataloader(batch_size=batch_size, test_batch_size=eval_batch_size,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad    \n",
    "adata_combined = ad.concat(adata_list, join='outer', axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D0-fm', 'D20-nr', 'P20-nr', 'D8-fm', 'D4-fm', 'P3-nr']\n",
       "Categories (6, object): ['D0-fm', 'D4-fm', 'D8-fm', 'D20-nr', 'P3-nr', 'P20-nr']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_combined.obs.library.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OKSM 3-element subsets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Generate all 3-element subsets\n",
    "subsets_oksm = list(combinations(OKSM, 4))\n",
    "\n",
    "best_model = model\n",
    "res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### General Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_gene_subset(expression, adata_var_index, model, gene_names_to_perturb, batch=4, n_masks=512, fraction=0.3):\n",
    "    \"\"\"\n",
    "    Perturb a desired subset of genes provided as a list of gene names.\n",
    "\n",
    "    Parameters:\n",
    "    - adata: AnnData object containing gene expression data\n",
    "    - model: PyTorch model used to sample perturbations\n",
    "    - gene_names_to_perturb: list of strings, names of genes to be perturbed\n",
    "    - batch: int, batch size for sampling perturbations\n",
    "    - n_masks: int, number of binary masks to generate for sampling\n",
    "    - fraction: float, fraction of genes to perturb in each mask\n",
    "\n",
    "    Returns:\n",
    "    - ref_cells: DataFrame, reference cell reconstructions\n",
    "    - pert_cells: DataFrame, perturbed cell reconstructions\n",
    "    \"\"\"\n",
    "    # Extract expression data\n",
    "    \n",
    "    expression_pert = expression.copy()\n",
    "    \n",
    "    # Find indices of genes to perturb\n",
    "    gene_indices_to_perturb = [adata_var_index.get_loc(gene) for gene in gene_names_to_perturb]\n",
    "    \n",
    "    # Apply overexpression to the selected genes\n",
    "    expression_pert[:, gene_indices_to_perturb] = expression_pert.max() * 10\n",
    "    \n",
    "    # Convert to tensors\n",
    "    #single_cell_ref = torch.tensor(expression)\n",
    "    single_cell_pert = torch.tensor(expression_pert)\n",
    "    \n",
    "    # Generate binary masks\n",
    "    n_genes = expression_pert.shape[1]\n",
    "    masks = generate_binary_array(n_masks, n_genes, fraction)\n",
    "    \n",
    "    # Sample reference cells\n",
    "    # ref_cells = []\n",
    "    # for i in range(0, masks.shape[0] - batch, batch):\n",
    "    #     res = sample_pert(model, Data(x=single_cell_ref), masks[i:i + batch]).detach().cpu().numpy()\n",
    "    #     ref_cells.append(res)\n",
    "    \n",
    "    # Sample perturbed cells\n",
    "    pert_cells = []\n",
    "    for i in range(0, masks.shape[0] - batch, batch):\n",
    "        res = sample_pert(model, Data(x=single_cell_pert), masks[i:i + batch]).detach().cpu().numpy()\n",
    "        pert_cells.append(res)\n",
    "    \n",
    "    # Convert results to DataFrames\n",
    "    # ref_cells = np.array(ref_cells).reshape(-1, n_genes)\n",
    "    # ref_cells = pd.DataFrame(ref_cells, columns=adata.var.index.tolist())\n",
    "\n",
    "    pert_cells = np.array(pert_cells).reshape(-1, n_genes)\n",
    "    pert_cells = pd.DataFrame(pert_cells, columns=adata_combined.var.index.tolist())\n",
    "    \n",
    "    return pert_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_combined = adata_combined[:,adata_combined.var.index.isin(to_perturb)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:36<00:00,  1.04it/s]\n",
      "100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n",
      "100%|██████████| 100/100 [01:35<00:00,  1.04it/s]\n",
      "100%|██████████| 100/100 [01:35<00:00,  1.04it/s]\n",
      "100%|██████████| 100/100 [01:36<00:00,  1.04it/s]\n",
      "100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "reconstructed_ref = True\n",
    "\n",
    "iterations = 1  # Number of random sampling iterations\n",
    "\n",
    "n_masks = 1024\n",
    "batch = 128\n",
    "fraction = 0.15\n",
    "libs = {}\n",
    "\n",
    "for library in ['D0-fm', 'D20-nr', 'P20-nr', 'D8-fm', 'D4-fm', 'P3-nr']:\n",
    "    if reconstructed_ref:\n",
    "        t = adata_combined[adata_combined.obs.library==library]\n",
    "        ref_cells_fin_list = []\n",
    "        for i in tqdm(range(100)):\n",
    "            choice = np.random.randint(low=0, high=t.X.shape[0])\n",
    "            expression = t.X[choice,:].toarray()\n",
    "            \n",
    "            single_cell_ref = torch.tensor(expression)\n",
    "            n_genes = single_cell_ref.shape[1]\n",
    "            masks = generate_binary_array(n_masks, n_genes, fraction)\n",
    "            \n",
    "            ref_cells = []\n",
    "            for i in range(0, masks.shape[0]-batch, batch):\n",
    "                res = sample_pert(model, Data(x=single_cell_ref), masks[i:i+batch]).detach().cpu().numpy()\n",
    "                ref_cells.append(res)\n",
    "                \n",
    "            ref_cells = np.array(ref_cells).reshape(-1, len(to_perturb))\n",
    "            ref_cells = pd.DataFrame(ref_cells, columns = adata_combined.var.index.tolist())\n",
    "            ref_cells_fin_list.append(ref_cells.mean())\n",
    "        libs[library] = np.mean(ref_cells_fin_list,axis=0)\n",
    "    else:\n",
    "        ref_cells = adata_combined[adata_combined.obs.library==library].X\n",
    "        libs[library] = ref_cells.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "cell_n = 10\n",
    "\n",
    "res = pd.DataFrame()\n",
    "counter = 0\n",
    "adata_var_index = adata_combined.var.index\n",
    "t = adata_combined[adata_combined.obs.condition=='ctrl']\n",
    "\n",
    "batch_size = 128\n",
    "n_masks = 1024\n",
    "frac_masked = 0.15\n",
    "for s in subsets_oksm+[OKSM]:\n",
    "    genes = [s]\n",
    "    name = '_'.join(genes[0])\n",
    "    for i in tqdm(range(iterations)):        \n",
    "        choice = cell_n #np.random.randint(low=0, high=t.X.shape[0])\n",
    "        expression = t.X[choice,:].toarray()\n",
    "    \n",
    "        single_cell = perturb_gene_subset(expression, adata_var_index, model, s, batch=batch_size, n_masks=n_masks, fraction=frac_masked).values\n",
    "        \n",
    "        for lib in libs:\n",
    "            res.loc[f'{name}_{choice}', f'euclid_distance_{lib}'] = np.mean([np.linalg.norm(libs[lib] - single_cell[i]) for i in range(single_cell.shape[0])])\n",
    "            res.loc[f'{name}_{choice}', f'pearson_corr_{lib}'] = np.mean([np.corrcoef(libs[lib], single_cell[i])[0,1] for i in range(single_cell.shape[0])])\n",
    "        res.to_csv('./save/ipsc_perturbation_sampling_search_oksm_1024_masks_RECON_REF_multi_cells_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res = pd.DataFrame()\n",
    "# # Perturbation Search\n",
    "# if 1==1:\n",
    "#     for s in tqdm(subsets):\n",
    "#         genes = [s]\n",
    "#         name = '_'.join(genes[0])\n",
    "#         single_cell = perturb_gene_subset(expression, adata_var_index, model, s, batch=batch_size, n_masks=n_masks, fraction=frac_masked).values\n",
    "#         #res_express.loc[name,pert_data.adata.var.index[gene_mask]] = single_cell\n",
    "        \n",
    "#         choice = np.random.randint(low=0, high=t.X.shape[0])\n",
    "#         expression = t.X[choice,:].toarray()\n",
    "    \n",
    "#         for lib in libs:\n",
    "#             res.loc[name, f'euclid_distance_{lib}'] = np.mean([np.linalg.norm(libs[lib] - single_cell[i]) for i in range(single_cell.shape[0])])\n",
    "#             res.loc[name, f'pearson_corr_{lib}'] = np.mean([np.corrcoef(libs[lib], single_cell[i])[0,1] for i in range(single_cell.shape[0])])\n",
    "#         counter+=1\n",
    "#         clear_output()\n",
    "#         if counter>=100:\n",
    "#             res.to_csv('./save/ipsc_perturbation_sampling_search_1024_masks_RECON_REF_multi_cells.csv')\n",
    "#             counter = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 517/50000 [11:33<18:26:12,  1.34s/it]"
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame()\n",
    "\n",
    "# Perturbation Search\n",
    "if 1 == 1:\n",
    "    for s in tqdm(subsets):\n",
    "        genes = [s]\n",
    "        name = '_'.join(genes[0])\n",
    "        mean_euclid_distance = {lib: [] for lib in libs}\n",
    "        mean_pearson_corr = {lib: [] for lib in libs}\n",
    "\n",
    "        for _ in range(iterations):\n",
    "            choice = cell_n #np.random.randint(low=0, high=t.X.shape[0])\n",
    "            expression = t.X[choice, :].toarray()\n",
    "            single_cell = perturb_gene_subset(expression, adata_var_index, model, s, batch=batch_size, n_masks=n_masks, fraction=frac_masked).values\n",
    "\n",
    "            for lib in libs:\n",
    "                euclid_distances = [np.linalg.norm(libs[lib] - single_cell[i]) for i in range(single_cell.shape[0])]\n",
    "                pearson_corrs = [np.corrcoef(libs[lib], single_cell[i])[0, 1] for i in range(single_cell.shape[0])]\n",
    "                mean_euclid_distance[lib].append(np.mean(euclid_distances))\n",
    "                mean_pearson_corr[lib].append(np.mean(pearson_corrs))\n",
    "\n",
    "        # Compute mean of means over iterations for robustness\n",
    "        for lib in libs:\n",
    "            res.loc[name, f'euclid_distance_{lib}'] = np.mean(mean_euclid_distance[lib])\n",
    "            res.loc[name, f'pearson_corr_{lib}'] = np.mean(mean_pearson_corr[lib])\n",
    "\n",
    "        counter += 1\n",
    "        clear_output()\n",
    "        if counter >= 100:\n",
    "            res.to_csv('./save/ipsc_perturbation_sampling_search_1024_masks_RECON_REF_multi_cells_v2.csv')\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Real data relationshiops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = pd.DataFrame(index = libs.keys(), columns = libs.keys())\n",
    "for lib1 in libs:\n",
    "    for lib2 in libs:\n",
    "        pearson_corrs = np.corrcoef(libs[lib1], libs[lib2])[0, 1]\n",
    "        corrs.loc[lib1,lib2] = pearson_corrs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "order = ['D0-fm','D4-fm','D8-fm','D20-nr','P3-nr','P20-nr']\n",
    "sns.heatmap(corrs.loc[order, order].astype(float))\n",
    "plt.savefig('./save/reprog_corr_heatmap.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libs[lib1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
